# 1
### (a)
Inflexible is best because simple models have enough data to learn the relationship with a large sample size. Also, flexible methods can overfit with few predictors.

### (b)
Flexible is best because it can capture complex relationships with higher dimensional data. Inflexible models are likely to underfit the data given the small amount of data.

### (c)
Flexible is best because it can capture non-linear relationships. Inflexible is too simple and will most likely underfit our data if it's non-linear.

### (d)
Inflexible is best because it generalizes noisy data with high variance. Flexible methods will likely overfit the data.

# 2
### (a)
i. False, provided that the GPA is not high, college graduates can generally earn more than high school graduates on average, as their base salary is \$35k higher.
ii. False, if the graduate's GPA is 4.0, our model predicts high school graduates will earn more than college graduates as the negative coefficient of the interaction effect between GPA and level outweighs the base salary boost of college graduates.
iii. True, when the GPA is high enough, the negative coefficient of the interaction effect between GPA and level outweighs the base salary boost for college graduates, resulting in high school graduates earning more on average.
iv. False, when the GPA is high enough, the negative coefficient of the interaction effect between GPA and level outweighs the base salary boost for college graduates, resulting in college graduates earning less on average.

### (b)
The model is as follows:
$$\hat Y = 50 + 20X_1 + 0.07X_2 + 35X_3 + 0.01X_1X_2 - 10X_1X_3$$

Given our inputs:
$$\hat Y = 50 + 20 * 4.0 + 0.07 * 110 + 35 + 0.01 * 4.0 * 110 - 10 * 4.0$$
$$\hat Y = `r 50 + 20 * 4.0 + 0.07 * 110 + 35 + 0.01 * 4.0 * 110 - 10 * 4.0`$$

### (c)
False, although the coefficient for the interaction effect appears small, we cannot claim an interaction effect without conducting a statistical significance test.

# 3
The RSS formula is as follows:

$$RSS = \sum (Y_i - \hat Y_i)^2$$

We are assuming that the data follows a polynomial relationship for all parts.

a. Generally, we would expect the training RSS of our cubic regression to appear lower than our linear regression, as it is more flexible and has more polynomial terms. Due to the randomness of $\epsilon$, the cubic regression will likely overfit and capture random noise when training, as the true relationship is linear.
b. Because of overfitting, we expect the testing RSS of our cubic regression to be higher than our linear regression.
c. If the data is close to linear, we apply the reason from part (a), and we expect the training RSS of our cubic regression to appear lower than the linear regression. If the data is far from linear, then cubic regression is likely a better fit, resulting in a lower training RSS than our linear regression.
d. If the true relationship is close to linear, then apply the answer from part (b). However, if the true relationship is far from linear, then then we would expect test RSS of our cubic regression to be lower than our linear regression, as our linear regression likely underfitted the model.

# 4
Let:

* \( S_{YY} = \sum Y_i^2 \) (Total Sum of Squares)
* \( S_{\text{res}} = \sum (Y_i - \hat{Y}_i)^2 \) (Residual Sum of Squares)
* \( S_{\text{reg}} = S_{YY} - S_{\text{res}} \) (Regression Sum of Squares)

Given:
\[
R^2 = \frac{\text{Explained Variance}}{\text{Total Variance}} = \frac{S_{YY} - S_{\text{res}}}{S_{YY}}
\]

Then:
\[
R^2 = 1 - \frac{S_{\text{res}}}{S_{YY}}.
\]

The simple linear regression equation is:

\[
\hat{Y} = \beta_1 X
\]

where \( \beta_1 \) is estimated as:

\[
\hat{\beta}_1 = \frac{\sum X_i Y_i}{\sum X_i^2} = \frac{S_{XY}}{S_{XX}}.
\]

Then the fitted values are:

\[
\hat{Y}_i = \hat{\beta}_1 X_i = \frac{S_{XY}}{S_{XX}} X_i.
\]

The total sum of squares is:

\[
S_{YY} = \sum Y_i^2.
\]

The regression sum of squares is:

\[
S_{\text{reg}} = \sum (\hat{Y}_i)^2 = \sum \left( \frac{S_{XY}}{S_{XX}} X_i \right)^2.
\]

Since:

\[
S_{\text{reg}} = \frac{S_{XY}^2}{S_{XX}^2} \sum X_i^2 = \frac{S_{XY}^2}{S_{XX}},
\]

we substitute into \( R^2 \):

\[
R^2 = \frac{S_{\text{reg}}}{S_{YY}} = \frac{S_{XY}^2}{S_{XX} S_{YY}}.
\]

By definition, the Pearson correlation coefficient \( r \) is:

\[
r = \frac{S_{XY}}{\sqrt{S_{XX} S_{YY}}}.
\]

Squaring both sides:

\[
r^2 = \frac{S_{XY}^2}{S_{XX} S_{YY}}.
\]

This is exactly the expression for \( R^2 \), so:

\[
R^2 = r^2.
\]

TODO: ?? Verify the proof is correct and figure out the tricks used.

# 5
```{r}
# Load required packages
library(ISLR)
library(ggplot2)

# Load the Auto dataset
data("Auto")
```

### (a)
```{r}
# (a) Fit simple linear regression: mpg ~ horsepower
lm_fit <- lm(mpg ~ horsepower, data = Auto)
lm_summary <- summary(lm_fit)
lm_summary

# (a) iv. make prediction
test_data <- data.frame(horsepower = 98)
prediction <- predict(lm_fit, newdata = test_data)[1]

# (a) iv. get the 95% confidence and prediction interval
conf_interval <- predict(lm_fit, newdata = test_data, interval = "confidence")
pred_interval <- predict(lm_fit, newdata = test_data, interval = "prediction")
```

i. Yes, because $p << 0.001$, indicating there is a significant relationship between horsepower and mpg.
ii. The $R^2$ value is around `r round(lm_summary$r.squared, 4)` which means that `r round(lm_summary$r.squared, 4) * 100`\% of variability in mpg is explained by horsepower.
iii. The relationship is negative since the coefficient for horsepower is $`r round(lm_summary$coefficients["horsepower", "Estimate"], 4)` < 0$.
iv. The prediction for 98hp is `r round(prediction, 4)`mpg. The 95\% confidence interval is $[`r round(conf_interval[2], 4)`, `r round(conf_interval[3], 4)`]$. The 95\% prediction interval is $[`r round(pred_interval[2], 4)`, `r round(pred_interval[3], 4)`]$.

### (b)
```{r}
# (b) Scatterplot with regression line
ggplot(Auto, aes(x = horsepower, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "MPG vs Horsepower",
       x = "Horsepower",
       y = "Miles Per Gallon (MPG)")
```

### (c)
```{r}
# (c) Diagnostic plots
par(mfrow = c(2, 2))  # Show all 4 plots in one window
plot(lm_fit)
```

i. The residuals vs fitted shows that the errors are not white noise. The data exhibits a pattern, showing that there may be an uncaptured underlying relationship. Utilizing a more flexible model or transforming the data may help.
ii. The data appears normal on the chart, as the points mostly corresopnd with the line. However, we cannot conclude normality without a formal test.
iii. The constant variance assumption is violated as there appears to be more residual variance for higher fitted values.
iv. It appears that some points above leverage $0.01$ may be more influential and skewing the regression. Data points with $|\text{standardized residual}| \geq 2$ may also skew the regression. Removing outliers may help.
